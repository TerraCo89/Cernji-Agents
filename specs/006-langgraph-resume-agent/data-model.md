# Data Model: LangGraph Resume Agent

**Date**: 2025-10-23
**Feature**: LangGraph Resume Agent (006)
**Purpose**: Define workflow state schemas, node interfaces, and data entities

## Overview

The LangGraph implementation introduces new workflow state entities while maintaining compatibility with existing data entities. State management is handled by LangGraph's type system, and all database entities remain unchanged.

---

## Workflow State Schemas

### ApplicationWorkflowState

**Purpose**: Root state for complete job application workflow (analyze → tailor → cover letter → portfolio)

**Schema** (Pydantic):
```python
from pydantic import BaseModel, Field, HttpUrl
from typing import Optional, List
from datetime import datetime

class ApplicationWorkflowState(BaseModel):
    """State for complete job application workflow"""

    # Input
    job_url: HttpUrl
    workflow_id: str = Field(default_factory=lambda: str(uuid.uuid4()))

    # Workflow metadata
    started_at: datetime = Field(default_factory=datetime.utcnow)
    current_node: Optional[str] = None
    completed_nodes: List[str] = Field(default_factory=list)

    # Caching metadata
    cached: bool = False
    cache_source: Optional[str] = None

    # Step outputs
    job_analysis: Optional[dict] = None
    tailored_resume: Optional[str] = None
    cover_letter: Optional[str] = None
    portfolio_examples: Optional[List[dict]] = None

    # Error tracking
    errors: List[str] = Field(default_factory=list)
    warnings: List[str] = Field(default_factory=list)

    # Performance tracking
    node_durations: dict[str, float] = Field(default_factory=dict)
    total_duration: Optional[float] = None
```

**Validation Rules**:
- `job_url` must be valid HTTP/HTTPS URL
- `workflow_id` must be unique UUID
- `errors` list accumulates failures without stopping workflow
- `node_durations` tracks performance per node for SC-003 verification

**State Transitions**:
1. `check_cache` → populate if cached, or proceed to `analyze_job`
2. `analyze_job` → on success, proceed to `tailor_resume`; on failure, add error and proceed
3. `tailor_resume` → on success, proceed to `generate_cover_letter`; on failure, add error and proceed
4. `generate_cover_letter` → on success, proceed to `find_portfolio_examples`; on failure, add error and proceed
5. `find_portfolio_examples` → terminal node, finalize state
6. All nodes → update `completed_nodes`, `current_node`, `node_durations`

---

### JobAnalysisWorkflowState

**Purpose**: State for standalone job analysis workflow

**Schema** (Pydantic):
```python
class JobAnalysisWorkflowState(BaseModel):
    """State for job analysis workflow"""

    # Input
    job_url: HttpUrl
    workflow_id: str = Field(default_factory=lambda: str(uuid.uuid4()))

    # Workflow metadata
    started_at: datetime = Field(default_factory=datetime.utcnow)
    cached: bool = False

    # Output
    job_analysis: Optional[dict] = None

    # Error tracking
    errors: List[str] = Field(default_factory=list)

    # Performance
    duration_ms: Optional[float] = None
```

**Validation Rules**:
- `duration_ms` must be <15000 for new analysis, <3000 for cached (SC-003)
- `job_analysis` must match existing JobAnalysis schema when present

---

### ResumeTailoringWorkflowState

**Purpose**: State for standalone resume tailoring workflow

**Schema** (Pydantic):
```python
class ResumeTailoringWorkflowState(BaseModel):
    """State for resume tailoring workflow"""

    # Input
    job_url: HttpUrl
    workflow_id: str = Field(default_factory=lambda: str(uuid.uuid4()))

    # Workflow metadata
    started_at: datetime = Field(default_factory=datetime.utcnow)
    cached: bool = False

    # Dependencies (from previous steps or cache)
    job_analysis: dict  # Required input
    master_resume: dict  # Loaded from DAL

    # Output
    tailored_resume: Optional[str] = None
    keywords_integrated: Optional[List[str]] = None

    # Error tracking
    errors: List[str] = Field(default_factory=list)

    # Performance
    duration_ms: Optional[float] = None
```

**Validation Rules**:
- `job_analysis` must be present (required dependency)
- `master_resume` must be loaded from DAL before node execution
- `duration_ms` must be <20000 (SC-003)

---

### WorkflowCheckpoint

**Purpose**: LangGraph checkpoint metadata for state persistence

**Schema** (TypedDict - used by LangGraph internally):
```python
from typing import TypedDict

class WorkflowCheckpoint(TypedDict):
    """LangGraph checkpoint structure"""
    thread_id: str           # Unique workflow execution ID
    checkpoint_id: str       # Auto-generated by LangGraph
    state: dict              # Serialized ApplicationWorkflowState
    next_node: str | None    # Next node to execute (for resumption)
    created_at: float        # Unix timestamp
```

**Storage**:
- Persisted to `data/workflow_checkpoints.db` by SqliteSaver
- Separate from `data/resume_agent.db` to avoid schema coupling
- Automatically managed by LangGraph (no manual CRUD)

**Lifecycle**:
1. Checkpoint created after each node completes
2. Retrieved on workflow resumption via `thread_id`
3. Garbage collected after workflow completion (configurable retention)

---

## Node Interfaces

All LangGraph nodes follow a consistent function signature:

```python
from typing import Callable

NodeFunction = Callable[[ApplicationWorkflowState], dict]

# Example node
def analyze_job_node(state: ApplicationWorkflowState) -> dict:
    """
    Analyze job posting and extract structured data.

    Args:
        state: Current workflow state

    Returns:
        Partial state update (merged with existing state)
    """
    try:
        # Call existing DAL function
        analysis = data_read_job_analysis(
            company=extract_company(state.job_url),
            job_title=extract_title(state.job_url)
        )

        return {
            "job_analysis": analysis,
            "completed_nodes": state.completed_nodes + ["analyze_job"],
            "current_node": None,
            "node_durations": {
                **state.node_durations,
                "analyze_job": time.time() - state.started_at
            }
        }
    except Exception as e:
        return {
            "errors": state.errors + [f"Job analysis failed: {str(e)}"],
            "completed_nodes": state.completed_nodes + ["analyze_job"],
            "current_node": None
        }
```

**Node Responsibilities**:
1. Execute workflow step logic
2. Update state with results or errors
3. Track performance metrics (`node_durations`)
4. Update workflow progress metadata (`completed_nodes`, `current_node`)

**Node Naming Convention**:
- `check_{resource}_cache` - Cache validation nodes
- `{action}_{resource}` - Primary action nodes (e.g., `analyze_job`, `tailor_resume`)
- `finalize_{workflow}` - Terminal nodes that aggregate results

---

## Existing Data Entities (No Changes)

The following entities from the existing implementation remain unchanged:

### JobAnalysis

```python
class JobAnalysis(BaseModel):
    company: str
    job_title: str
    requirements: List[str]
    skills: List[str]
    responsibilities: List[str]
    salary_range: Optional[str]
    location: str
    keywords: List[str]
    url: str
    fetched_at: datetime
```

**Storage**: `data/resume_agent.db` (existing schema)
**Access**: via `data_read_job_analysis()` and `data_write_job_analysis()` (existing DAL)

---

### TailoredResume

```python
class TailoredResume(BaseModel):
    company: str
    job_title: str
    content: str  # Markdown/PDF content
    keywords_integrated: List[str]
    created_at: datetime
    metadata: Optional[dict]
```

**Storage**: `job-applications/{company}_{job_title}/tailored-resume.md` (existing file structure)
**Access**: via `data_read_tailored_resume()` and `data_write_tailored_resume()` (existing DAL)

---

### CoverLetter

```python
class CoverLetter(BaseModel):
    company: str
    job_title: str
    content: str  # Markdown content
    created_at: datetime
    metadata: Optional[dict]
```

**Storage**: `job-applications/{company}_{job_title}/cover-letter.md` (existing file structure)
**Access**: via `data_read_cover_letter()` and `data_write_cover_letter()` (existing DAL)

---

### PortfolioExample

```python
class PortfolioExample(BaseModel):
    example_id: int
    title: str
    description: str
    technologies: List[str]
    repository_url: str
    code_snippet: Optional[str]
    created_at: datetime
```

**Storage**: `data/resume_agent.db` (existing schema)
**Access**: via `data_search_portfolio_examples()` (existing DAL)

---

## Data Flow Diagram

```
┌──────────────────────────────────────────────────────────────┐
│                  MCP Tool Layer (FastMCP)                    │
│  complete_application_workflow(), analyze_job_posting(), ... │
└────────────────────────┬─────────────────────────────────────┘
                         │
                         ▼
┌──────────────────────────────────────────────────────────────┐
│                LangGraph Workflow Layer                       │
│  StateGraph → Nodes → Conditional Edges → Checkpoints        │
│  (ApplicationWorkflowState, JobAnalysisWorkflowState, ...)   │
└────────────────────────┬─────────────────────────────────────┘
                         │
                         ▼
┌──────────────────────────────────────────────────────────────┐
│          Data Access Layer (Existing DAL functions)          │
│  data_read_*(), data_write_*(), data_search_*()              │
└────────────────────────┬─────────────────────────────────────┘
                         │
                         ▼
┌──────────────────────────────────────────────────────────────┐
│                    Storage Layer                             │
│  data/resume_agent.db (SQLite)                               │
│  data/workflow_checkpoints.db (SqliteSaver)                  │
│  job-applications/ (file system)                             │
│  resumes/ (file system)                                      │
└──────────────────────────────────────────────────────────────┘
```

**Key Principles**:
1. **MCP tools** are the public interface (backward compatibility)
2. **LangGraph workflows** handle orchestration and state management
3. **DAL functions** remain unchanged (reused from existing implementation)
4. **Storage layer** unchanged (existing schema preserved)

---

## Schema Versioning

**Current Version**: 1.0.0 (initial LangGraph implementation)

**Versioning Policy**:
- **Major**: Breaking changes to ApplicationWorkflowState structure
- **Minor**: New optional fields added to state
- **Patch**: Bug fixes, validation rule updates

**Migration Strategy**:
- Workflow state is ephemeral (checkpoints cleared after completion)
- No data migration needed for workflow state schema changes
- Existing data entities (JobAnalysis, TailoredResume, etc.) remain v1.0.0

---

## Validation Summary

| Entity | Validation Method | Enforced At |
|--------|-------------------|-------------|
| ApplicationWorkflowState | Pydantic | Node entry/exit |
| JobAnalysisWorkflowState | Pydantic | Node entry/exit |
| ResumeTailoringWorkflowState | Pydantic | Node entry/exit |
| JobAnalysis | Pydantic (existing) | DAL layer |
| TailoredResume | Pydantic (existing) | DAL layer |
| CoverLetter | Pydantic (existing) | DAL layer |
| PortfolioExample | Pydantic (existing) | DAL layer |

**Validation Strategy**: All state updates validated by Pydantic before LangGraph state merge. DAL layer validates existing entities (unchanged from current implementation).
