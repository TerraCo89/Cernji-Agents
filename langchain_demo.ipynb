{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Agent: LangChain/LangGraph Implementation Demo\n",
    "\n",
    "This notebook demonstrates multi-agent orchestration using LangChain and LangGraph for automated job applications.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "Job URL ‚Üí Job Analyzer ‚Üí Portfolio Finder ‚Üí Resume Writer ‚Üí Cover Letter Writer ‚Üí Save Files\n",
    "          (Node 1)       (Node 2)          (Node 3)       (Node 4)            (Node 5)\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "- Multi-agent collaboration with LangGraph StateGraph\n",
    "- Structured output using Pydantic models\n",
    "- Tool calling for GitHub and web searches\n",
    "- Conditional routing based on match scores\n",
    "- State persistence with checkpointing\n",
    "- RAG for semantic resume search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install dependencies:\n",
    "```bash\n",
    "pip install -r requirements-langchain.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import TypedDict, Optional, List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key is set\n",
    "assert os.getenv(\"ANTHROPIC_API_KEY\"), \"Please set ANTHROPIC_API_KEY in .env\"\n",
    "print(\"‚úì Environment configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: State Definition\n",
    "\n",
    "LangGraph uses TypedDict to define the state that flows through the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Sequence\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "# Pydantic model for structured output\n",
    "class JobAnalysis(BaseModel):\n",
    "    \"\"\"Structured job posting data\"\"\"\n",
    "    company: str\n",
    "    job_title: str\n",
    "    location: str\n",
    "    required_qualifications: List[str] = Field(default_factory=list)\n",
    "    keywords: List[str] = Field(default_factory=list)\n",
    "    candidate_profile: str\n",
    "\n",
    "# State flows through the graph\n",
    "class ApplicationState(TypedDict):\n",
    "    \"\"\"State for job application workflow\"\"\"\n",
    "    job_url: str\n",
    "    job_analysis: Optional[JobAnalysis]\n",
    "    match_score: Optional[float]\n",
    "    tailored_resume: Optional[str]\n",
    "    cover_letter: Optional[str]\n",
    "    messages: Sequence[BaseMessage]\n",
    "\n",
    "print(\"‚úì State defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Initialize LLM with Structured Output\n",
    "\n",
    "LangChain's `with_structured_output()` ensures type-safe LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Claude with structured output\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=4096\n",
    ")\n",
    "\n",
    "# LLM that returns JobAnalysis objects\n",
    "job_analysis_llm = llm.with_structured_output(JobAnalysis)\n",
    "\n",
    "# Test it\n",
    "test_result = job_analysis_llm.invoke(\n",
    "    \"\"\"Extract job info from this posting:\n",
    "    \n",
    "    Company: Acme Corp\n",
    "    Position: Senior Python Engineer\n",
    "    Location: Remote\n",
    "    Requirements: 5+ years Python, LangChain, FastAPI\n",
    "    We're looking for an experienced engineer to build AI applications.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(f\"‚úì Structured output works: {test_result.company} - {test_result.job_title}\")\n",
    "print(f\"  Keywords: {', '.join(test_result.keywords[:3])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Define Agent Nodes\n",
    "\n",
    "Each node is a specialized agent that performs a specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def job_analyzer_node(state: ApplicationState) -> Dict[str, Any]:\n",
    "    \"\"\"Node 1: Analyze job posting\"\"\"\n",
    "    print(\"  üîç Analyzing job posting...\")\n",
    "    \n",
    "    # In production, would fetch real job posting\n",
    "    # For demo, use mock data\n",
    "    mock_job_content = f\"\"\"\n",
    "    Job Posting: Senior LangChain Engineer\n",
    "    Company: AI Startup Inc.\n",
    "    Location: Remote\n",
    "    \n",
    "    Requirements:\n",
    "    - 5+ years Python development\n",
    "    - Experience with LangChain and LangGraph\n",
    "    - RAG implementation experience\n",
    "    - Vector database knowledge (Pinecone, FAISS)\n",
    "    - Strong understanding of LLM prompting\n",
    "    \n",
    "    We're building next-generation AI applications using LangChain.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Analyze with structured output\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Extract job requirements and keywords from job postings.\"),\n",
    "        (\"human\", \"Analyze this job posting:\\n\\n{content}\\n\\nURL: {url}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | job_analysis_llm\n",
    "    job_analysis = chain.invoke({\n",
    "        \"content\": mock_job_content,\n",
    "        \"url\": state[\"job_url\"]\n",
    "    })\n",
    "    \n",
    "    # Calculate match score (simplified)\n",
    "    match_score = 0.85\n",
    "    \n",
    "    return {\n",
    "        \"job_analysis\": job_analysis,\n",
    "        \"match_score\": match_score,\n",
    "        \"messages\": state.get(\"messages\", []) + [\n",
    "            AIMessage(content=f\"‚úì Analyzed {job_analysis.company} - {job_analysis.job_title}\")\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def resume_writer_node(state: ApplicationState) -> Dict[str, Any]:\n",
    "    \"\"\"Node 2: Tailor resume\"\"\"\n",
    "    print(\"  üìù Writing tailored resume...\")\n",
    "    \n",
    "    job_analysis = state[\"job_analysis\"]\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an expert resume writer. Create ATS-optimized resumes.\"),\n",
    "        (\"human\", \"\"\"Create a tailored resume for:\n",
    "        \n",
    "Company: {company}\n",
    "Role: {job_title}\n",
    "Required Skills: {skills}\n",
    "\n",
    "Highlight relevant experience and incorporate these keywords: {keywords}\n",
    "Keep it concise and ATS-friendly.\n",
    "\"\"\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm\n",
    "    resume = chain.invoke({\n",
    "        \"company\": job_analysis.company,\n",
    "        \"job_title\": job_analysis.job_title,\n",
    "        \"skills\": \", \".join(job_analysis.required_qualifications[:5]),\n",
    "        \"keywords\": \", \".join(job_analysis.keywords[:10])\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        \"tailored_resume\": resume.content,\n",
    "        \"messages\": state.get(\"messages\", []) + [\n",
    "            AIMessage(content=\"‚úì Tailored resume created\")\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def cover_letter_writer_node(state: ApplicationState) -> Dict[str, Any]:\n",
    "    \"\"\"Node 3: Generate cover letter\"\"\"\n",
    "    print(\"  ‚úâÔ∏è Generating cover letter...\")\n",
    "    \n",
    "    job_analysis = state[\"job_analysis\"]\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an expert cover letter writer. Create compelling narratives.\"),\n",
    "        (\"human\", \"\"\"Write a cover letter for {company} - {job_title}.\n",
    "        \n",
    "Show enthusiasm, cultural fit, and relevant experience.\n",
    "3-4 paragraphs, professional but personable tone.\n",
    "\"\"\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm\n",
    "    cover_letter = chain.invoke({\n",
    "        \"company\": job_analysis.company,\n",
    "        \"job_title\": job_analysis.job_title\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        \"cover_letter\": cover_letter.content,\n",
    "        \"messages\": state.get(\"messages\", []) + [\n",
    "            AIMessage(content=\"‚úì Cover letter generated\")\n",
    "        ]\n",
    "    }\n",
    "\n",
    "print(\"‚úì Nodes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Build the Workflow Graph\n",
    "\n",
    "LangGraph's StateGraph defines the agent workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create workflow\n",
    "workflow = StateGraph(ApplicationState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"analyze_job\", job_analyzer_node)\n",
    "workflow.add_node(\"write_resume\", resume_writer_node)\n",
    "workflow.add_node(\"write_cover_letter\", cover_letter_writer_node)\n",
    "\n",
    "# Define edges (workflow sequence)\n",
    "workflow.add_edge(START, \"analyze_job\")\n",
    "workflow.add_edge(\"analyze_job\", \"write_resume\")\n",
    "workflow.add_edge(\"write_resume\", \"write_cover_letter\")\n",
    "workflow.add_edge(\"write_cover_letter\", END)\n",
    "\n",
    "# Compile with memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "print(\"‚úì Workflow compiled\")\n",
    "\n",
    "# Visualize the graph (requires pygraphviz)\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"  (Graph visualization requires pygraphviz: {e})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Execute the Workflow\n",
    "\n",
    "Run the multi-agent workflow end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize state\n",
    "initial_state = ApplicationState(\n",
    "    job_url=\"https://example.com/jobs/senior-langchain-engineer\",\n",
    "    messages=[HumanMessage(content=\"Apply to job\")]\n",
    ")\n",
    "\n",
    "# Execute workflow\n",
    "print(\"üöÄ Starting job application workflow...\\n\")\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"demo-001\"}}\n",
    "final_state = app.invoke(initial_state, config=config)\n",
    "\n",
    "print(\"\\n‚úÖ Workflow complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "job_analysis = final_state[\"job_analysis\"]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"JOB ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Company: {job_analysis.company}\")\n",
    "print(f\"Title: {job_analysis.job_title}\")\n",
    "print(f\"Location: {job_analysis.location}\")\n",
    "print(f\"Match Score: {final_state['match_score']:.0%}\")\n",
    "print(f\"\\nRequired Skills:\")\n",
    "for skill in job_analysis.required_qualifications[:5]:\n",
    "    print(f\"  - {skill}\")\n",
    "print(f\"\\nKeywords: {', '.join(job_analysis.keywords[:10])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TAILORED RESUME (Preview)\")\n",
    "print(\"=\"*60)\n",
    "print(final_state[\"tailored_resume\"][:500] + \"...\\n\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"COVER LETTER (Preview)\")\n",
    "print(\"=\"*60)\n",
    "print(final_state[\"cover_letter\"][:500] + \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Advanced - Conditional Routing\n",
    "\n",
    "Route workflow based on match score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_apply(state: ApplicationState) -> str:\n",
    "    \"\"\"Decide whether to proceed based on match score.\"\"\"\n",
    "    match_score = state.get(\"match_score\", 0)\n",
    "    \n",
    "    if match_score >= 0.7:\n",
    "        print(f\"  ‚úì Match score {match_score:.0%} - proceeding with application\")\n",
    "        return \"write_resume\"\n",
    "    else:\n",
    "        print(f\"  ‚úó Match score {match_score:.0%} - skipping application\")\n",
    "        return \"end\"\n",
    "\n",
    "# Create workflow with conditional routing\n",
    "workflow_conditional = StateGraph(ApplicationState)\n",
    "workflow_conditional.add_node(\"analyze_job\", job_analyzer_node)\n",
    "workflow_conditional.add_node(\"write_resume\", resume_writer_node)\n",
    "workflow_conditional.add_node(\"write_cover_letter\", cover_letter_writer_node)\n",
    "\n",
    "workflow_conditional.add_edge(START, \"analyze_job\")\n",
    "\n",
    "# Conditional edge based on match score\n",
    "workflow_conditional.add_conditional_edges(\n",
    "    \"analyze_job\",\n",
    "    should_apply,\n",
    "    {\n",
    "        \"write_resume\": \"write_resume\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow_conditional.add_edge(\"write_resume\", \"write_cover_letter\")\n",
    "workflow_conditional.add_edge(\"write_cover_letter\", END)\n",
    "\n",
    "app_conditional = workflow_conditional.compile(checkpointer=MemorySaver())\n",
    "\n",
    "print(\"‚úì Conditional workflow created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Bonus - RAG for Resume Search\n",
    "\n",
    "Semantic search over career history using vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Mock career history (in production, load from YAML)\n",
    "career_docs = [\n",
    "    Document(page_content=\"\"\"Company: Tech Corp\n",
    "Position: Senior Python Engineer\n",
    "Technologies: Python, FastAPI, PostgreSQL, Docker\n",
    "Achievements:\n",
    "- Built microservices architecture serving 1M+ users\n",
    "- Reduced API latency by 60%\n",
    "- Mentored team of 5 junior engineers\n",
    "\"\"\"),\n",
    "    Document(page_content=\"\"\"Company: AI Startup\n",
    "Position: ML Engineer\n",
    "Technologies: Python, LangChain, OpenAI, FAISS, Pinecone\n",
    "Achievements:\n",
    "- Implemented RAG system with 95% accuracy\n",
    "- Built multi-agent orchestration with LangGraph\n",
    "- Deployed production LLM applications\n",
    "\"\"\")\n",
    "]\n",
    "\n",
    "# Only run if OpenAI API key is available\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    # Create embeddings and vector store\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    vectorstore = FAISS.from_documents(career_docs, embeddings)\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "    \n",
    "    # Search for relevant experience\n",
    "    query = \"LangChain and RAG experience\"\n",
    "    results = retriever.invoke(query)\n",
    "    \n",
    "    print(f\"\\nüîç Search: '{query}'\\n\")\n",
    "    for i, doc in enumerate(results, 1):\n",
    "        print(f\"Result {i}:\")\n",
    "        print(doc.page_content[:200] + \"...\\n\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping RAG demo - OPENAI_API_KEY not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "1. ‚úÖ **Multi-agent orchestration** with LangGraph StateGraph\n",
    "2. ‚úÖ **Structured output** using Pydantic models\n",
    "3. ‚úÖ **State management** with TypedDict\n",
    "4. ‚úÖ **Conditional routing** based on business logic\n",
    "5. ‚úÖ **Checkpointing** for resumable workflows\n",
    "6. ‚úÖ **RAG implementation** with vector embeddings\n",
    "7. ‚úÖ **LCEL chains** for composable LLM operations\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Deploy as LangServe API\n",
    "- Add LangSmith tracing\n",
    "- Implement streaming responses\n",
    "- Add human-in-the-loop for review\n",
    "- Scale with production vector database\n",
    "\n",
    "## Portfolio Value\n",
    "\n",
    "This implementation showcases production-ready patterns for LangChain roles:\n",
    "- Architectural understanding of agent systems\n",
    "- Type safety and validation\n",
    "- Stateful workflow management\n",
    "- RAG and vector search\n",
    "- Real-world use case\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
